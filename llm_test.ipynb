{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### This is file is used to generate German language data for testing. And for running the tests for Noise Robustness, Negative Rejection and Information Integration."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung:\n",
    "## RAG Objekt initialisieren, mit dem Daten für die Tests generiert werden:\n",
    "Es wird ein RAG Objekt initialisiert, das dafür benötigt wird, die Dokumente, auf denen zu Testen gewünscht ist, als Vector-Embeddings zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y tensorboard tensorboard-data-server tensorboard-plugin-wit tensorflow tensorflow-estimator tensorflow-io-gcs-filesystem\n",
    "# !pip uninstall -y nvidia-cublas-cu11 nvidia-cublas-cu12 nvidia-cuda-cupti-cu11 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu11 nvidia-cuda-nvrtc-cu12 nvidia-cuda-runtime-cu11 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu11 nvidia-cudnn-cu12 nvidia-cufft-cu11 nvidia-cufft-cu12 nvidia-curand-cu11 nvidia-curand-cu12 nvidia-cusolver-cu11 nvidia-cusolver-cu12 nvidia-cusparse-cu11 nvidia-cusparse-cu12 nvidia-nccl-cu11 nvidia-nccl-cu12 nvidia-nvjitlink-cu12 nvidia-nvtx-cu11 nvidia-nvtx-cu12  \n",
    "# !pip install --user -r \"retrieval_augmented_generation/requirements.txt\"\n",
    "# !pip install -U torch~=2.2.1\n",
    "# !pip install -U bitsandbytes~=0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from retrieval_augmented_generation.rag import RAG\n",
    "from retrieval_augmented_generation.configs import RetrievalConfig, TextGenerationConfig\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import copy\n",
    "\n",
    "generate_anything_new = \"\"\n",
    "while generate_anything_new != \"1\" and generate_anything_new != \"2\":\n",
    "    generate_anything_new = input(\"Neue oder teilweise neue Test-Daten generieren (1), oder bereits generierte Test-Daten einlesen (2)?\")\n",
    "clear_output()\n",
    "if generate_anything_new == \"1\":\n",
    "    generate_anything_new = True\n",
    "else:\n",
    "    generate_anything_new = False\n",
    "\n",
    "    \n",
    "if generate_anything_new:\n",
    "    \n",
    "    rag = RAG(files_directory=\"../Wikipedia\")\n",
    "\n",
    "    retrieval_config = RetrievalConfig(collection_name=\"wikipedia\")\n",
    "\n",
    "    rag._init_nomodel(retrieval_config=retrieval_config)\n",
    "    \n",
    "    while True:\n",
    "        do_update = input(\"Vektor-Embeddings aktualisieren? (Y/N)\")\n",
    "        if do_update ==\"Y\" or do_update == \"y\":\n",
    "            rag.update_files()\n",
    "            break\n",
    "        elif do_update == \"N\" or do_update == \"n\":\n",
    "            break\n",
    "else:\n",
    "    rag = RAG(files_directory=\"../Wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen zum Speichern der Testdaten definieren: \n",
    "Damit die Tests vergleichbar sind, können die verwendeten Fragen, Antworten und Kontexte gespeichert werden. Dafür werden zwei Funktionen definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_test_data():\n",
    "    # read file:\n",
    "    try:\n",
    "        with open('llm_test_data.json', 'r') as file:\n",
    "            llm_test_data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        llm_test_data = {}\n",
    "    \n",
    "    # create all not yet existant lists:\n",
    "    if 'qas' not in llm_test_data:\n",
    "        llm_test_data['qas'] = []\n",
    "    if 'qas_with_noise' not in llm_test_data:\n",
    "        llm_test_data['qas_with_noise'] = []\n",
    "    if 'qas_two_answers' not in llm_test_data:\n",
    "        llm_test_data['qas_two_answers'] = []\n",
    "    if 'qas_with_wrong_context' not in llm_test_data:\n",
    "        llm_test_data['qas_with_wrong_context'] = []\n",
    "    \n",
    "    # convert all documents to Document object\n",
    "    for qas_name in llm_test_data:\n",
    "        for (i_qa_pair, _) in enumerate(llm_test_data[qas_name]):\n",
    "            for (i_doc, doc) in enumerate(llm_test_data[qas_name][i_qa_pair]['context']):\n",
    "                llm_test_data[qas_name][i_qa_pair]['context'][i_doc] = Document(page_content=doc['page_content'], metadata=doc['metadata'])\n",
    "    \n",
    "    return llm_test_data\n",
    "    \n",
    "def persist_llm_test_data(llm_test_data):\n",
    "    # convert all documents to dict object:\n",
    "    for qas_name in llm_test_data:\n",
    "        for (i_qa_pair, _) in enumerate(llm_test_data[qas_name]):\n",
    "            for (i_doc, doc) in enumerate(llm_test_data[qas_name][i_qa_pair]['context']):\n",
    "                if type(doc) is not dict:\n",
    "                    dictionary = {}\n",
    "                    dictionary['page_content'] = doc.page_content\n",
    "                    dictionary['metadata'] = doc.metadata\n",
    "                    llm_test_data[qas_name][i_qa_pair]['context'][i_doc] = dictionary\n",
    "    \n",
    "    # write file:\n",
    "    with open('llm_test_data.json', 'w') as file:\n",
    "        json.dump(llm_test_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragen als Ausgangspunkt für die Tests generieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_test_data = get_llm_test_data()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"XXXX\"\n",
    "\n",
    "if generate_anything_new:\n",
    "    generate_new_questions = \"\"\n",
    "    while generate_new_questions != \"1\" and generate_new_questions != \"2\":\n",
    "        generate_new_questions = input(\"Neue Fragen generieren (1), oder bereits generierte Fragen einlesen (2)?\")\n",
    "    clear_output()\n",
    "    if generate_new_questions == \"1\":\n",
    "        generate_new_questions = True\n",
    "    else:\n",
    "        generate_new_questions = False\n",
    "\n",
    "if generate_anything_new and generate_new_questions:\n",
    "    \n",
    "    # an even number of questions is optimal\n",
    "    n_questions = 100\n",
    "\n",
    "    random_docs = rag._get_random_docs(int(n_questions*2))\n",
    "\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Sie sind ein hilfreicher Assistent, der dabei hilft, Fragen für eine Open-Book-Prüfung zu erstellen. Sie erhalten einen Textausschnitt, zu dem Sie eine Frage generieren, die sich mithilfe des Textausschnitts beantworten lassen muss. Es muss auch möglich sein, dass die beantwortende Person die Antwort aus einer anderen Quelle mit den gleichen Informationen recherchiert. In der Frage darf deshalb nicht davon ausgegangen werden, dass die beantwortende Person den gleichen Textausscnitt vorliegen hat. Beispiele für gute Themen für Fragen sind zum Beispiel Namen, Abkürzungen, Zahlen, Adressen oder andere nachprüfbare Fakten. Die Beispiele sind nicht zu eng zu sehen, die Antwort sollte aber maximal vier Wort lang sein. Geben Sie das Ergebnis wie in diesem Beispiel an:\\n*Frage*Welche Farbe haben Bananen?*Antwort*Gelb\\nEs ist möglich, dass der gegebene Textausschnitt keine zufriedenstellende Grundlage bietet, um ein Frage-Antwort-Paar zu generieren, etwa weil der Text zu unspezifisch ist, keinen sinnvollen Inhalt hat, die beantwortende Person nicht wissen könnte, worauf sich die Frage bezieht, oder andere Gründe. Es ist für Sie besser, bei Unsicherheit kein Frage-Antwort-Paar zu generieren. Antworten Sie in diesem Fall folgendermaßen:\\n*Textausschnitt ungenügend*\"),\n",
    "        (\"human\", \"{document}\"),\n",
    "    ])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    qas = []\n",
    "    while len(qas) < n_questions:\n",
    "        if len(random_docs) == 0:\n",
    "            random_docs = rag._get_random_docs(int(n_questions*2))\n",
    "\n",
    "        doc = random_docs.pop()\n",
    "        gpt_reply = llm_chain.invoke({'document':doc.page_content})['text']\n",
    "        if \"*Textausschnitt ungenügend*\" in gpt_reply:\n",
    "            print(\"Kontext ungenügend\\n\\nKontext:\\n\", doc.page_content, \"\\n\")\n",
    "            print(\"-----------------------------------------------------------\\n\")\n",
    "        else:\n",
    "            qa_pair = gpt_reply.replace(\"*Frage*\",\"\").split(\"*Antwort*\")\n",
    "            qa_pair = {'question': qa_pair[0], 'answer': qa_pair[1], 'context':[doc]}\n",
    "            print(\"Frage:\\n\", qa_pair['question'], \"\\n\\nAntwort:\\n\", qa_pair['answer'], \"\\n\\nKontext:\\n\", qa_pair['context'][0].page_content, \"\\n\")\n",
    "            while True:\n",
    "                is_good = input(\"Zufriedenstellend? (Y/N)\")\n",
    "                if is_good ==\"Y\" or is_good == \"y\":\n",
    "                    qas.append(qa_pair)\n",
    "                    clear_output()\n",
    "                    break\n",
    "                elif is_good == \"N\" or is_good == \"n\":\n",
    "                    clear_output()\n",
    "                    break\n",
    "        \n",
    "        \n",
    "    print(\"Erfolgreich\", len(qas), \"neue Frage-Antwort-Paare generiert\")    \n",
    "    llm_test_data['qas'] = qas\n",
    "    persist_llm_test_data(llm_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Robustness Test:\n",
    "Fragen mit relevantem Kontext und zufälligem Kontext kombinieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_contexts = 5\n",
    "\n",
    "llm_test_data = get_llm_test_data()\n",
    "\n",
    "if generate_anything_new:\n",
    "    \n",
    "    generate_new_noise = \"\"\n",
    "    while generate_new_noise != \"1\" and generate_new_noise != \"2\":\n",
    "        generate_new_noise = input(\"Neue Noise generieren (1), oder bereits generierte Fragen und Noise einlesen (2)?\")\n",
    "    clear_output()\n",
    "    if generate_new_noise == \"1\":\n",
    "        generate_new_noise = True\n",
    "    else:\n",
    "        generate_new_noise = False\n",
    "\n",
    "if generate_anything_new and generate_new_noise:\n",
    "    \n",
    "    qas_with_noise = copy.deepcopy(llm_test_data['qas'])\n",
    "\n",
    "    for qa_pair in qas_with_noise:\n",
    "        random_docs = rag._get_random_docs(n_max_contexts-1)\n",
    "        for doc in random_docs:\n",
    "            qa_pair['context'].append(doc)\n",
    "    \n",
    "    print(\"Erfolgreich\", len(qas_with_noise), \"Frage-Antwort-Paare mit zufälligem Kontext kombiniert\")  \n",
    "    llm_test_data['qas_with_noise'] = qas_with_noise\n",
    "    persist_llm_test_data(llm_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Integration Test:\n",
    "\n",
    "Zwei Frage-Antwort-Paare zu einem Frage-Antwort-Paar kombinieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_test_data = get_llm_test_data()\n",
    "\n",
    "if generate_anything_new:\n",
    "    generate_new_question_combinations = \"\"\n",
    "    while generate_new_question_combinations != \"1\" and generate_new_question_combinations != \"2\":\n",
    "        generate_new_question_combinations = input(\"Neue Kombinationen aus Zwei Frage-Antwort-Paaren generieren (1), oder bereits generierte einlesen (2)?\")\n",
    "    clear_output()\n",
    "    if generate_new_question_combinations == \"1\":\n",
    "        generate_new_question_combinations = True\n",
    "    else:\n",
    "        generate_new_question_combinations = False\n",
    "\n",
    "if generate_anything_new and generate_new_question_combinations:\n",
    "    \n",
    "    qas_two_answers_tmp = copy.deepcopy(llm_test_data['qas'])\n",
    "    \n",
    "    # remove last element, if len is an odd number\n",
    "    if len(qas_two_answers_tmp) % 2 == 1:\n",
    "        qas_two_answers_tmp = qas_two_answers_tmp[:-1]\n",
    "    \n",
    "    qas_two_answers = []\n",
    "    qas_two_answers_2ndhalf = []\n",
    "    for (i, x) in enumerate(qas_two_answers_tmp):\n",
    "        if i % 2 != 0:\n",
    "            qas_two_answers.append(x)\n",
    "        else:\n",
    "            qas_two_answers_2ndhalf.append(x)\n",
    "    \n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Sie sind ein hilfreicher Assistent, der dabei hilft, zwei in zwei Sätzen formulierte Fragen, in einem Satz zu formulieren. Es muss unbedingt der Inhalt beider Fragen vollständig vorhanden bleiben. Antworten Sie nur mit dem resultierenden Satz.\"),\n",
    "        (\"human\", \"{question1}\\n{question2}\"),\n",
    "    ])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    \n",
    "    for i_question in range(len(qas_two_answers)):\n",
    "        question_1 = qas_two_answers[i_question]['question']\n",
    "        answer_1 = qas_two_answers[i_question]['answer']\n",
    "        question_2 = qas_two_answers_2ndhalf[i_question]['question']\n",
    "        answer_2 = qas_two_answers_2ndhalf[i_question]['answer']\n",
    "        \n",
    "        qas_two_answers[i_question].pop('answer')\n",
    "        qas_two_answers[i_question]['answer1'] = answer_1\n",
    "        qas_two_answers[i_question]['answer2'] = answer_2\n",
    "        context2 = qas_two_answers_2ndhalf[i_question]['context'].pop()\n",
    "        qas_two_answers[i_question]['context'].append(context2)\n",
    "        \n",
    "        looking_for_good_question = True\n",
    "        while looking_for_good_question:\n",
    "            gpt_reply = llm_chain.invoke({'question1': question_1, 'question2': question_2})['text']\n",
    "            print(\"Frage 1:\\n\", question_1, \"\\nFrage 2:\\n\", question_2, \"\\n\\nkombinierte Frage:\\n\", gpt_reply, \"\\n\")\n",
    "            while True:\n",
    "                    is_good = input(\"Zufriedenstellend? (Y/N)\")\n",
    "                    if is_good ==\"Y\" or is_good == \"y\":\n",
    "                        qas_two_answers[i_question]['question'] = gpt_reply\n",
    "                        looking_for_good_question = False\n",
    "                        clear_output()\n",
    "                        break\n",
    "                    elif is_good == \"N\" or is_good == \"n\":\n",
    "                        clear_output()\n",
    "                        break\n",
    "        \n",
    "    print(\"Erfolgreich\", len(qas_two_answers), \"Mal zwei Frage-Antwort-Paare kombiniert\")  \n",
    "    llm_test_data['qas_two_answers'] = qas_two_answers\n",
    "    persist_llm_test_data(llm_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Negative Rejection Test:\n",
    "Fragen mit zufälligem Kontext kombinieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_test_data = get_llm_test_data()\n",
    "\n",
    "if generate_anything_new:\n",
    "    generate_new_context = \"\"\n",
    "    while generate_new_context != \"1\" and generate_new_context != \"2\":\n",
    "        generate_new_context = input(\"Neuen Kontext zu Fragen generieren (1), oder bereits generierte Fragen und Kontext einlesen (2)?\")\n",
    "    clear_output()\n",
    "    if generate_new_context == \"1\":\n",
    "        generate_new_context = True\n",
    "    else:\n",
    "        generate_new_context = False\n",
    "\n",
    "if generate_anything_new and generate_new_context:\n",
    "    \n",
    "    qas_with_wrong_context = copy.deepcopy(llm_test_data['qas'])\n",
    "\n",
    "    for qa_pair in qas_with_wrong_context:\n",
    "        qa_pair['context'].pop()\n",
    "        random_doc = rag._get_random_docs(1)[0]\n",
    "        qa_pair['context'].append(random_doc)\n",
    "    \n",
    "    print(\"Erfolgreich\", len(qas_with_wrong_context), \"Frage-Antwort-Paare mit zufälligem Kontext kombiniert\")  \n",
    "    llm_test_data['qas_with_wrong_context'] = qas_with_wrong_context\n",
    "    persist_llm_test_data(llm_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durchführung der Tests\n",
    "## Initialisierung eines RAG Objekts mit dem zu verwendenden Modell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  mistralai/Mistral-7B-Instruct-v0.3\n",
      "2 :  LeoLM/leo-mistral-hessianai-7b-chat\n",
      "3 :  flozi00/Mistral-7B-german-assistant-v4 - custom template\n",
      "4 :  OpenBuddy/openbuddy-mistral2-7b-v20.3-32k\n",
      "5 :  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "6 :  VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\n",
      "7 :  OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k\n",
      "8 :  meta-llama/Llama-2-7b-chat-hf\n",
      "9 :  flozi00/Llama-2-7b-german-assistant-v3\n",
      "10 :  meta-llama/Llama-2-13b-chat-hf\n",
      "11 :  flozi00/Llama-2-13b-german-assistant-v6\n",
      "12 :  ass-a2s/Llama-2-13b-chat-german\n",
      "13 :  meta-llama/Meta-Llama-3-8B-Instruct\n",
      "14 :  DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1\n",
      "15 :  VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct\n",
      "16 :  OpenBuddy/openbuddy-llama3-8b-v21.1-8k\n",
      "17 :  google/gemma-1.1-7b-it\n",
      "18 :  OpenBuddy/openbuddy-gemma-7b-v19.1-4k\n",
      "19 :  VAGOsolutions/SauerkrautLM-Gemma-7b\n",
      "20 :  VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct_new-template\n",
      "21 :  VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct_new-template\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Nr des gewünschten Text-Generation-Models: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init db 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533a05a9b90a4b159870a1b82029bb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## rag = RAG(files_directory=\"../Dokumente\")\n",
    "\n",
    "openai_or_huggingface = \"\"\n",
    "while openai_or_huggingface != \"1\" and openai_or_huggingface != \"2\":\n",
    "    openai_or_huggingface = input(\"Mit OpenAI-Modell initialisieren (1), oder mit Huggingface-Modell initialisieren (2)?\")\n",
    "clear_output(wait=True)\n",
    "\n",
    "models = []\n",
    "\n",
    "if openai_or_huggingface == \"1\":\n",
    "    models = {\n",
    "        \"gpt-3.5-turbo-0125\": TextGenerationConfig(text_generation_model_name=\"gpt-3.5-turbo-0125\"),\n",
    "        \"gpt-4o-2024-05-13\": TextGenerationConfig(text_generation_model_name=\"gpt-4o-2024-05-13\"),\n",
    "        \n",
    "        \"gpt-4o-2024-05-13_new-template\": TextGenerationConfig(text_generation_model_name=\"gpt-4o-2024-05-13\")\n",
    "    }\n",
    "elif openai_or_huggingface == \"2\":\n",
    "    models = {\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.3\": TextGenerationConfig(text_generation_model_name=\"mistralai/Mistral-7B-Instruct-v0.3\"),\n",
    "        \"LeoLM/leo-mistral-hessianai-7b-chat\": TextGenerationConfig(text_generation_model_name=\"LeoLM/leo-mistral-hessianai-7b-chat\"),\n",
    "        # can't get chat template to work:\n",
    "        \"flozi00/Mistral-7B-german-assistant-v4 - custom template\": TextGenerationConfig(text_generation_model_name=\"flozi00/Mistral-7B-german-assistant-v4\", custom_chat_template=\"{system-message} </s>### User: {user-message} </s>### Assistant:\"),\n",
    "        # can't get chat template to work:\n",
    "        \"OpenBuddy/openbuddy-mistral2-7b-v20.3-32k\": TextGenerationConfig(text_generation_model_name=\"OpenBuddy/openbuddy-mistral2-7b-v20.3-32k\", custom_chat_template=\"{system-message}\\n\\n{user-message}\\nAntwort:\"),\n",
    "        \n",
    "        \"mistralai/Mixtral-8x7B-Instruct-v0.1\": TextGenerationConfig(text_generation_model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"),\n",
    "        \"VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\": TextGenerationConfig(text_generation_model_name=\"VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\"),\n",
    "        \"OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k\": TextGenerationConfig(text_generation_model_name=\"OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k\", custom_chat_template=\"{system-message}\\n\\n{user-message}\\nAssistant:\"),\n",
    "        \n",
    "        \"meta-llama/Llama-2-7b-chat-hf\": TextGenerationConfig(text_generation_model_name=\"meta-llama/Llama-2-7b-chat-hf\"),\n",
    "        \"flozi00/Llama-2-7b-german-assistant-v3\": TextGenerationConfig(text_generation_model_name=\"flozi00/Llama-2-7b-german-assistant-v3\"),\n",
    "        \n",
    "        # wiederholter Abbruch bei >3 Kontext;\n",
    "        \"meta-llama/Llama-2-13b-chat-hf\": TextGenerationConfig(text_generation_model_name=\"meta-llama/Llama-2-13b-chat-hf\"),\n",
    "        \"flozi00/Llama-2-13b-german-assistant-v6\": TextGenerationConfig(text_generation_model_name=\"flozi00/Llama-2-13b-german-assistant-v6\"),\n",
    "        \"ass-a2s/Llama-2-13b-chat-german\": TextGenerationConfig(text_generation_model_name=\"ass-a2s/Llama-2-13b-chat-german\"),\n",
    "        \n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\": TextGenerationConfig(text_generation_model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"),\n",
    "        \"DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1\": TextGenerationConfig(text_generation_model_name=\"DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1\"),\n",
    "        \"VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct\": TextGenerationConfig(text_generation_model_name=\"VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct\"),\n",
    "        \"OpenBuddy/openbuddy-llama3-8b-v21.1-8k\": TextGenerationConfig(text_generation_model_name=\"OpenBuddy/openbuddy-llama3-8b-v21.1-8k\"),\n",
    "        \n",
    "        \"google/gemma-1.1-7b-it\": TextGenerationConfig(text_generation_model_name=\"google/gemma-1.1-7b-it\"),\n",
    "        # can't get chat template to work:\n",
    "        \"OpenBuddy/openbuddy-gemma-7b-v19.1-4k\": TextGenerationConfig(text_generation_model_name=\"OpenBuddy/openbuddy-gemma-7b-v19.1-4k\", custom_chat_template=\"{system-message}\\n\\n{user-message}\\nAssistant:\"),\n",
    "        # can't get chat template to work and stopping string would need to be manually added:\n",
    "        \"VAGOsolutions/SauerkrautLM-Gemma-7b\": TextGenerationConfig(text_generation_model_name=\"VAGOsolutions/SauerkrautLM-Gemma-7b\", custom_chat_template=\"{system-message}\\n\\n{user-message}\\nASSISTANT:\"),\n",
    "        \n",
    "        \"VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct_new-template\": TextGenerationConfig(text_generation_model_name=\"VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\"),\n",
    "        \"VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct_new-template\": TextGenerationConfig(text_generation_model_name=\"VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct\"),\n",
    "    }\n",
    "\n",
    "model_nr = 0\n",
    "\n",
    "\n",
    "while not (model_nr >= 1 and model_nr <= len(models)):\n",
    "    for (i, model_name) in enumerate(models):\n",
    "        print(i+1, \": \", model_name)\n",
    "    model_nr = int(input(\"Nr des gewünschten Text-Generation-Models:\"))\n",
    "model_name, text_generation_config = list(models.items())[model_nr-1]\n",
    "\n",
    "def initialize_model():\n",
    "    rag = RAG(files_directory=\"../Wikipedia\")\n",
    "    \n",
    "    if openai_or_huggingface == \"1\":\n",
    "        rag.init_openai(\n",
    "            open_ai_key=\"XXXX\",\n",
    "            embedding_model_name=\"text-embedding-ada-002\",\n",
    "            text_generation_config=text_generation_config\n",
    "        )\n",
    "    elif openai_or_huggingface == \"2\":\n",
    "        rag.init_huggingface(\n",
    "            hf_transformers_cache_dir=\"./../../hf_transformers_cache\",\n",
    "            hf_hub_api_key=\"hf_XXXX\",\n",
    "            retrieval_config=RetrievalConfig(\n",
    "                embedding_model_name=\"intfloat/multilingual-e5-base\",\n",
    "                embedding_query_template=\"{text}\",\n",
    "                retrieval_query_template=\"query:{question}\"\n",
    "            ),\n",
    "            text_generation_config=text_generation_config\n",
    "        )\n",
    "    return rag\n",
    "        \n",
    "rag = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse generiernen und manuell bewerten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if not (len(llm_test_data['qas']) == len(llm_test_data['qas_with_noise']) == len(llm_test_data['qas_two_answers'])*2 == len(llm_test_data['qas_with_wrong_context'])):\n",
    "    raise Exception(\"Fragen in unterschiedlicher Anzahl liegen vor. Test kann nicht durchgeführt werden.\")\n",
    "\n",
    "all_generated_answers = []\n",
    "\n",
    "for generate_or_rate in range(2):\n",
    "    \n",
    "    qas_with_noise_correct = {}\n",
    "    for i in range(n_max_contexts + 1)[1:]:\n",
    "        qas_with_noise_correct[i] = 0\n",
    "    qas_two_answers_correct = 0\n",
    "    qas_with_wrong_context_rejected = 0\n",
    "\n",
    "    for i_qa in range(len(llm_test_data['qas'])):\n",
    "        print((i_qa+1), \"/\", len(llm_test_data['qas']), \"\\n\")\n",
    "\n",
    "        for n_contexts in range(n_max_contexts + 1)[1:]:\n",
    "            if generate_or_rate == 0:\n",
    "                try:\n",
    "                    answer_given = rag._ask_with_custom_context(question=llm_test_data['qas_with_noise'][i_qa]['question'], context=llm_test_data['qas_with_noise'][i_qa]['context'][:n_contexts])\n",
    "                    # print(answer_given)\n",
    "                    answer_given = answer_given['answer']\n",
    "                    to_be_rated = {\n",
    "                        \"question\": llm_test_data[\"qas_with_noise\"][i_qa]['question'],\n",
    "                        \"correct_answer\": llm_test_data[\"qas_with_noise\"][i_qa]['answer'],\n",
    "                        \"given_answer\": answer_given\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    del rag\n",
    "                    to_be_rated = {\n",
    "                        \"question\": llm_test_data[\"qas_with_noise\"][i_qa]['question'],\n",
    "                        \"correct_answer\": llm_test_data[\"qas_with_noise\"][i_qa]['answer'],\n",
    "                        \"given_answer\": \"!!!TEXT GENERATION FAILED!!!\"\n",
    "                    }\n",
    "                if 'rag' not in locals() or 'rag' not in globals():\n",
    "                    print(\"initializing new object\")\n",
    "                    rag = initialize_model()\n",
    "                all_generated_answers.append(to_be_rated)\n",
    "            else:\n",
    "                to_be_rated = all_generated_answers.pop(0)\n",
    "            print(\"Frage:\\n\" , to_be_rated['question'], \"\\n\\nKorrekte Antwort:\\n\", to_be_rated['correct_answer'], \"\\n\\ngegebene Antwort:\\n\", to_be_rated['given_answer'], \"\\n\")\n",
    "            if generate_or_rate == 1:\n",
    "                while True:\n",
    "                    is_good = input(\"Ist die gegebene Antwort KORREKT? (Y/N)\")\n",
    "                    if is_good ==\"Y\" or is_good == \"y\":\n",
    "                        qas_with_noise_correct[n_contexts] = qas_with_noise_correct[n_contexts] + 1\n",
    "                        break\n",
    "                    elif is_good == \"N\" or is_good == \"n\":\n",
    "                        break\n",
    "            print(\"\\n-----------------------------------------------------------\\n\")\n",
    "\n",
    "        if i_qa % 2 == 1: \n",
    "            if generate_or_rate == 0:\n",
    "                try:\n",
    "                    answer_given = rag._ask_with_custom_context(question=llm_test_data['qas_two_answers'][int(i_qa/2)]['question'], context=llm_test_data['qas_two_answers'][int(i_qa/2)]['context'])['answer']\n",
    "                    to_be_rated = {\n",
    "                        \"question\": llm_test_data[\"qas_two_answers\"][int(i_qa/2)]['question'],\n",
    "                        \"correct_answer1\": llm_test_data[\"qas_two_answers\"][int(i_qa/2)]['answer1'],\n",
    "                        \"correct_answer2\": llm_test_data[\"qas_two_answers\"][int(i_qa/2)]['answer2'],\n",
    "                        \"given_answer\": answer_given\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    del rag\n",
    "                    rag = initialize_model()\n",
    "                    to_be_rated = {\n",
    "                        \"question\": llm_test_data[\"qas_with_noise\"][i_qa]['question'],\n",
    "                        \"correct_answer1\": llm_test_data[\"qas_two_answers\"][int(i_qa/2)]['answer1'],\n",
    "                        \"correct_answer2\": llm_test_data[\"qas_two_answers\"][int(i_qa/2)]['answer2'],\n",
    "                        \"given_answer\": \"!!!TEXT GENERATION FAILED!!!\"\n",
    "                    }\n",
    "                if 'rag' not in locals() or 'rag' not in globals():\n",
    "                    print(\"initializing new object\")\n",
    "                    rag = initialize_model()\n",
    "                all_generated_answers.append(to_be_rated)\n",
    "            else:\n",
    "                to_be_rated = all_generated_answers.pop(0)\n",
    "            print(\"Frage:\\n\" , to_be_rated['question'], \"\\n\\nKorrekte Antwort, erste Hälfte:\\n\", to_be_rated['correct_answer1'], \"\\n\\nKorrekte Antwort, zweite Hälfte:\\n\", to_be_rated['correct_answer2'], \"\\n\\ngegebene Antwort:\\n\", to_be_rated['given_answer'], \"\\n\")\n",
    "            if generate_or_rate == 1:\n",
    "                while True:\n",
    "                    is_good = input(\"Liefert die Antwort BEIDE Antworten KORREKT? (Y/N)\")\n",
    "                    if is_good ==\"Y\" or is_good == \"y\":\n",
    "                        qas_two_answers_correct = qas_two_answers_correct + 1\n",
    "                        break\n",
    "                    elif is_good == \"N\" or is_good == \"n\":\n",
    "                        break\n",
    "            print(\"\\n-----------------------------------------------------------\\n\")\n",
    "\n",
    "        if generate_or_rate == 0:\n",
    "            try:\n",
    "                answer_given = rag._ask_with_custom_context(question=llm_test_data['qas_with_wrong_context'][i_qa]['question'], context=llm_test_data['qas_with_wrong_context'][i_qa]['context'])['answer']\n",
    "                to_be_rated = {\n",
    "                    \"question\": llm_test_data[\"qas_with_wrong_context\"][i_qa]['question'],\n",
    "                    \"given_answer\": answer_given\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                del rag\n",
    "                rag = initialize_model()\n",
    "                to_be_rated = {\n",
    "                    \"question\": llm_test_data[\"qas_with_noise\"][i_qa]['question'],\n",
    "                    \"given_answer\": \"!!!TEXT GENERATION FAILED!!!\"\n",
    "                }\n",
    "            if 'rag' not in locals() or 'rag' not in globals():\n",
    "                print(\"initializing new object\")\n",
    "                rag = initialize_model()\n",
    "            all_generated_answers.append(to_be_rated)\n",
    "        else:\n",
    "            to_be_rated = all_generated_answers.pop(0)\n",
    "        print(\"Frage:\\n\" , to_be_rated['question'], \"\\n\\ngegebene Antwort:\\n\", to_be_rated['given_answer'], \"\\n\")\n",
    "        if generate_or_rate == 1:\n",
    "            while True:\n",
    "                is_good = input(\"Wird darauf hingewiesen, dass die Frage NICHT mit dem Kontext beantwortet werden kann? (Y/N)\")\n",
    "                if is_good ==\"Y\" or is_good == \"y\":\n",
    "                    qas_with_wrong_context_rejected = qas_with_wrong_context_rejected + 1\n",
    "                    break\n",
    "                elif is_good == \"N\" or is_good == \"n\":\n",
    "                    break\n",
    "        clear_output()\n",
    "        #KOMMT WIEDER WEG:\n",
    "        with open('mixtral_to_be_rated.json', 'w') as file:\n",
    "            json.dump(to_be_rated, file)\n",
    "            print(\"Daten gespeichert.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    with open('llm_test_results.json', 'r') as file:\n",
    "        llm_test_results = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    llm_test_results = {}\n",
    "    \n",
    "llm_test_results[model_name] = {}\n",
    "llm_test_results[model_name]['noise_robustness'] = {}\n",
    "for i in range(n_max_contexts):\n",
    "    llm_test_results[model_name]['noise_robustness'][i+1] = qas_with_noise_correct[i+1] / len(llm_test_data['qas'])\n",
    "llm_test_results[model_name]['information_integration'] = qas_two_answers_correct / (len(llm_test_data['qas']) / 2)\n",
    "llm_test_results[model_name]['negative_rejection'] = qas_with_wrong_context_rejected / len(llm_test_data['qas'])\n",
    "\n",
    "noise_robustness_string = \"\" \n",
    "for i in range(n_max_contexts):\n",
    "    noise_robustness_string = noise_robustness_string + str(i) + \"x Noise: \" + str(llm_test_results[model_name]['noise_robustness'][i+1]) + \" \"\n",
    "print(\"Noise Robustness: \", noise_robustness_string)\n",
    "print(\"Information Integration:\", str(llm_test_results[model_name]['information_integration']))\n",
    "print(\"Negative Rejection:\", str(llm_test_results[model_name]['negative_rejection']))\n",
    "\n",
    "with open('llm_test_results.json', 'w') as file:\n",
    "    json.dump(llm_test_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
